Rank,Model name,Model type,Avg. Spearman,Std. error of diff. To best score*,Description,References
1,TranceptEVE M,Hybrid model,0.516,0,TranceptEVE Medium model (Tranception Medium & retrieved EVE model),"Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. & (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop."
2,Tranception M,Hybrid model,0.509,0.001,Tranception Medium model (300M params) with retrieval,"Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML."
3,Tranception S,Hybrid model,0.477,0.021,Tranception Small model (85M params) with retrieval,"Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML."
4,TranceptEVE L,Hybrid model,0.466,0.023,TranceptEVE Large model (Tranception Large & retrieved EVE model),"Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. & (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop."
5,Tranception L,Hybrid model,0.464,0.022,Tranception Large model (700M params) with retrieval,"Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML."
6,RITA (ensemble),Protein language model,0.446,0.028,Ensemble of the 4 RITA models,"Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789."
7,Tranception L no retrieval,Protein language model,0.43,0.023,Tranception Large model (700M params) without retrieval,"Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML."
8,RITA M,Protein language model,0.426,0.037,RITA medium model (300M params),"Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789."
9,RITA L,Protein language model,0.414,0.027,RITA large model (680M params),"Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789."
10,Wavenet,Alignment-based model,0.412,0.077,Wavenet model,"Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12."
11,Unirep evotuned,Hybrid model,0.409,0.058,Unirep model w/ evotuning,"Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8."
12,Progen2 (ensemble),Protein language model,0.407,0.116,Ensemble of the 5 Progen2 models,"Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. "
13,RITA XL,Protein language model,0.406,0.035,RITA xlarge model (1.2B params),"Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789."
14,Progen2 XL,Protein language model,0.396,0.042,Progen2 xlarge model (6.4B params),"Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. "
15,Progen2 Base,Protein language model,0.384,0.122,Progen2 base model (760M params),"Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. "
16,Progen2 L,Protein language model,0.369,0.125,Progen2 large model (2.7B params),"Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. "
17,Progen2 M,Protein language model,0.358,0.125,Progen2 medium model (760M params),"Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. "
18,Progen2 S,Protein language model,0.352,0.118,Progen2 small model (150M params),"Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. "
19,RITA S,Protein language model,0.346,0.056,RITA small model (85M params),"Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789."
20,ProtGPT2,Protein language model,0.115,0.099,ProtGPT2 model,"Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13."
21,Unirep,Protein language model,0.111,0.068,Unirep model,"Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8."
