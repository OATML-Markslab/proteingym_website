Model_rank,Model_name,Model type,Average_Spearman,Bootstrap_standard_error_Spearman,Average_Spearman_fold_random_5,Average_Spearman_fold_modulo_5,Average_Spearman_fold_contiguous_5,Function_Activity,Function_Binding,Function_Expression,Function_OrganismalFitness,Function_Stability,Low_MSA_depth,Medium_MSA_depth,High_MSA_depth,Taxa_Human,Taxa_Other_Eukaryote,Taxa_Prokaryote,Taxa_Virus,References,Model details
1,Kermut,Structure & sequence embedding,0.657,0.0,0.745,0.633,0.593,0.605,0.611,0.67,0.582,0.817,0.74,0.612,0.655,0.705,0.666,0.703,0.623,"<a href='https://openreview.net/forum?id=jM9atrvUii'>Groth, P. M., Kerrn, M. H., Olsen, L., Salomon, J., & Boomsma, W. (2024). Kermut: Composite kernel regression for protein variant effects. Thirty-Eighth Conference on Neural Information Processing Systems</a>",Kermut GP
2,ProteinNPT,Sequence embedding,0.619,0.009,0.741,0.588,0.529,0.59,0.541,0.631,0.558,0.776,0.707,0.589,0.618,0.673,0.633,0.666,0.602,"<a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ProteinNPT Model
3,MSA Transformer Embeddings,Sequence embedding,0.581,0.013,0.67,0.562,0.513,0.57,0.479,0.583,0.528,0.746,0.686,0.571,0.58,0.654,0.607,0.638,0.555,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer Embeddings
4,Tranception Embeddings,Sequence embedding,0.548,0.008,0.681,0.525,0.439,0.507,0.489,0.582,0.514,0.65,0.603,0.535,0.547,0.552,0.57,0.564,0.56,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception Embeddings
5,ESM-1v Embeddings,Sequence embedding,0.535,0.014,0.614,0.514,0.479,0.488,0.46,0.552,0.467,0.71,0.652,0.465,0.537,0.567,0.585,0.599,0.437,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v Embeddings
6,TranceptEVE + One-Hot Encodings,One-hot Encoding,0.474,0.012,0.552,0.435,0.435,0.505,0.428,0.475,0.469,0.493,0.502,0.478,0.469,0.488,0.494,0.46,0.481,"[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",TranceptEVE + One-Hot Encodings
7,Tranception + One-Hot Encodings,One-hot Encoding,0.459,0.012,0.55,0.413,0.415,0.483,0.406,0.476,0.449,0.483,0.497,0.453,0.451,0.471,0.48,0.446,0.461,"[1] Original model: <a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",Tranception + One-Hot Encodings
8,MSA_Transformer + One-Hot Encodings,One-hot Encoding,0.452,0.014,0.54,0.409,0.407,0.488,0.372,0.467,0.437,0.495,0.501,0.441,0.452,0.491,0.464,0.459,0.453,"[1] Original model: <a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",MSA Transformer + One-Hot Encodings
9,ESM-1v + One-Hot Encodings,One-hot Encoding,0.452,0.015,0.565,0.394,0.396,0.46,0.416,0.473,0.414,0.496,0.526,0.381,0.436,0.47,0.48,0.467,0.35,"[1] Original model: <a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a> [2] Extension: <a href='https://openreview.net/forum?id=AwzbQVuDBk'>Notin, P., Weitzman, R., Marks, D. S., & Gal, Y. (2023). ProteinNPT: Improving protein property prediction and design with non-parametric transformers. Thirty-Seventh Conference on Neural Information Processing Systems</a>",ESM-1v + One-Hot Encodings
10,DeepSequence + One-Hot Encodings,One-hot Encoding,0.437,0.017,0.523,0.393,0.395,0.471,0.407,0.415,0.423,0.47,0.483,0.419,0.428,0.457,0.464,0.441,0.393,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",DeepSequence + One-Hot Encodings
11,One-Hot Encodings,One-hot Encoding,0.221,0.014,0.582,0.022,0.059,0.214,0.198,0.234,0.19,0.269,0.247,0.206,0.218,0.233,0.221,0.223,0.228,"<a href='https://www.nature.com/articles/s41587-021-01146-5'>Hsu, C., Nisonoff, H., Fannjiang, C. et al. Learning protein fitness models from evolutionary and assay-labeled data. Nat Biotechnol 40, 1114–1122 (2022). https://doi.org/10.1038/s41587-021-01146-5</a>",One-Hot Encodings
