<!DOCTYPE html>
<html>
<head>
	<meta http-equiv="Content-type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
	<link rel="shortcut icon" type="image/png" href="/media/images/favicon.png">
	<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="http://www.datatables.net/rss.xml">
	<link rel="stylesheet" type="text/css" href="/media/css/site-examples.css?_=11229a4cc52ab488c3d6ed72e1ec231e1">
	<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.12.1/css/jquery.dataTables.min.css">
	<style type="text/css" class="init">
	</style>
	<script type="text/javascript" language="javascript" src="https://code.jquery.com/jquery-3.5.1.js"></script>
	<script type="text/javascript" language="javascript" src="https://cdn.datatables.net/1.12.1/js/jquery.dataTables.min.js"></script>
	<script type="text/javascript" language="javascript" src="../resources/demo.js"></script>
	<script type="text/javascript" class="init">
	

$(document).ready(function () {
	$('#example').DataTable({
        lengthMenu: [
            [15, 30, -1],
            [15, 30, 'All'],
        ],
        columnDefs: [
        {
            targets: [0,3,4,5,6,7,8,9,10,11,12,13,14,15,16],
            className: 'dt-center'
        },
        {
            targets: [1,2,17,18],
            className: 'dt-left'
        }
      ]
    });
});


	</script>
</head>
<body class="wide comments example">
	<a name="top" id="top"></a>
	<div class="fw-background">
		<div></div>
	</div>
	<div class="fw-container">
		<div class="fw-body">
			<div class="content">
                <div>
					<table id="example" class="hover stripe nowrap" style="width:100%">
                        <thead>
                            <tr>
                                <th rowspan="2">Rank</th>
                                <th rowspan="2">Model name</th>
                                <th rowspan="2">Model type</th>
                                <th rowspan="2">Avg. Spearman</th>
                                <th rowspan="2">Std. error of diff.<br> to best score<sup>*</sup></th>
                                <th colspan="3">Spearman by MSA depth</th>
                                <th colspan="4">Spearman by taxon</th>
                                <th colspan="5">Spearman by mutation depth</th>
                                <th colspan="3">Model details</th>
                            </tr>
                            <tr>
                                <th title="Field #3">Low depth</th>
                                <th title="Field #4">Medium depth</th>
                                <th title="Field #5">High depth</th>
                                <th title="Field #6">Human</th>
                                <th title="Field #7">Other Eukaryote</th>
                                <th title="Field #8">Prokaryote</th>
                                <th title="Field #9">Virus</th>
                                <th title="Field #10">1</th>
                                <th title="Field #11">2</th>
                                <th title="Field #12">3</th>
                                <th title="Field #13">4</th>
                                <th title="Field #15">5+</th>
                                
                                <th title="Field #17">Description</th>
                                <th title="Field #18">References</th>
                            </tr>
                        </thead>
                        
<tbody>
    <tr>
      <th>1</th>
      <td>TranceptEVE L</td>
      <td>Hybrid model</td>
      <td>0.472</td>
      <td>0.000</td>
      <td>0.460</td>
      <td>0.463</td>
      <td>0.508</td>
      <td>0.440</td>
      <td>0.518</td>
      <td>0.514</td>
      <td>0.454</td>
      <td>0.472</td>
      <td>0.444</td>
      <td>0.456</td>
      <td>0.389</td>
      <td>0.464</td>
      <td>TranceptEVE Large model (Tranception Large & retrieved EVE model)</td>
      <td><a href='https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1?rss=1'>Notin, P., Van Niekerk, L., Kollasch, A., Ritter, D., Gal, Y. & Marks, D.S. &  (2022). TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction. NeurIPS, LMRL workshop.</a></td>
    </tr>
    <tr>
      <th>2</th>
      <td>GEMME</td>
      <td>Alignment-based model</td>
      <td>0.459</td>
      <td>0.007</td>
      <td>0.440</td>
      <td>0.451</td>
      <td>0.497</td>
      <td>0.423</td>
      <td>0.504</td>
      <td>0.488</td>
      <td>0.458</td>
      <td>0.455</td>
      <td>0.397</td>
      <td>0.384</td>
      <td>0.35</td>
      <td>0.437</td>
      <td>GEMME model</td>
      <td><a href='https://pubmed.ncbi.nlm.nih.gov/31406981/'>Laine, Ã‰., Karami, Y., & Carbone, A. (2019). GEMME: A Simple and Fast Global Epistatic Model Predicting Mutational Effects. Molecular Biology and Evolution, 36, 2604 - 2619.</a></td>
    </tr>
    <tr>
      <th>3</th>
      <td>EVE (ensemble)</td>
      <td>Alignment-based model</td>
      <td>0.449</td>
      <td>0.005</td>
      <td>0.423</td>
      <td>0.441</td>
      <td>0.498</td>
      <td>0.408</td>
      <td>0.499</td>
      <td>0.500</td>
      <td>0.435</td>
      <td>0.45</td>
      <td>0.409</td>
      <td>0.405</td>
      <td>0.351</td>
      <td>0.429</td>
      <td>EVE model (ensemble of 5 independently-trained models)</td>
      <td><a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a></td>
    </tr>
    <tr>
      <th>4</th>
      <td>Tranception L</td>
      <td>Hybrid model</td>
      <td>0.446</td>
      <td>0.005</td>
      <td>0.447</td>
      <td>0.436</td>
      <td>0.472</td>
      <td>0.417</td>
      <td>0.503</td>
      <td>0.478</td>
      <td>0.429</td>
      <td>0.442</td>
      <td>0.427</td>
      <td>0.44</td>
      <td>0.37</td>
      <td>0.461</td>
      <td>Tranception Large model (700M params) with retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>5</th>
      <td>VESPA</td>
      <td>Protein language model</td>
      <td>0.444</td>
      <td>0.010</td>
      <td>0.426</td>
      <td>0.426</td>
      <td>0.516</td>
      <td>0.411</td>
      <td>0.505</td>
      <td>0.529</td>
      <td>0.384</td>
      <td>0.439</td>
      <td>0.381</td>
      <td>0.351</td>
      <td>0.283</td>
      <td>0.34</td>
      <td>VESPA model</td>
      <td><a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a></td>
    </tr>
    <tr>
      <th>6</th>
      <td>EVE (single)</td>
      <td>Alignment-based model</td>
      <td>0.443</td>
      <td>0.005</td>
      <td>0.415</td>
      <td>0.434</td>
      <td>0.494</td>
      <td>0.400</td>
      <td>0.489</td>
      <td>0.493</td>
      <td>0.433</td>
      <td>0.442</td>
      <td>0.406</td>
      <td>0.407</td>
      <td>0.35</td>
      <td>0.431</td>
      <td>EVE model (single seed)</td>
      <td><a href='https://www.nature.com/articles/s41586-021-04043-8'>Frazer, J., Notin, P., Dias, M., Gomez, A.N., Min, J.K., Brock, K.P., Gal, Y., & Marks, D.S. (2021). Disease variant prediction with deep generative models of evolutionary data. Nature.</a></td>
    </tr>
    <tr>
      <th>7</th>
      <td>MSA Transformer (ensemble)</td>
      <td>Hybrid model</td>
      <td>0.432</td>
      <td>0.012</td>
      <td>0.398</td>
      <td>0.426</td>
      <td>0.485</td>
      <td>0.394</td>
      <td>0.487</td>
      <td>0.498</td>
      <td>0.398</td>
      <td>0.433</td>
      <td>0.374</td>
      <td>0.401</td>
      <td>0.351</td>
      <td>0.418</td>
      <td>MSA Transformer (ensemble of 5 MSA samples)</td>
      <td><a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a></td>
    </tr>
    <tr>
      <th>8</th>
      <td>Tranception M</td>
      <td>Hybrid model</td>
      <td>0.430</td>
      <td>0.008</td>
      <td>0.430</td>
      <td>0.430</td>
      <td>0.430</td>
      <td>0.420</td>
      <td>0.481</td>
      <td>0.426</td>
      <td>0.421</td>
      <td>0.428</td>
      <td>0.314</td>
      <td>0.273</td>
      <td>0.304</td>
      <td>0.405</td>
      <td>Tranception Medium model (300M params) with retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>9</th>
      <td>DeepSequence (ensemble)</td>
      <td>Alignment-based model</td>
      <td>0.421</td>
      <td>0.009</td>
      <td>0.393</td>
      <td>0.403</td>
      <td>0.498</td>
      <td>0.400</td>
      <td>0.493</td>
      <td>0.492</td>
      <td>0.348</td>
      <td>0.419</td>
      <td>0.394</td>
      <td>0.407</td>
      <td>0.353</td>
      <td>0.436</td>
      <td>DeepSequence model (ensemble of 5 independently-trained models)</td>
      <td><a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a></td>
    </tr>
    <tr>
      <th>10</th>
      <td>MSA Transformer (single)</td>
      <td>Hybrid model</td>
      <td>0.421</td>
      <td>0.013</td>
      <td>0.382</td>
      <td>0.416</td>
      <td>0.472</td>
      <td>0.381</td>
      <td>0.488</td>
      <td>0.487</td>
      <td>0.382</td>
      <td>0.422</td>
      <td>0.358</td>
      <td>0.388</td>
      <td>0.339</td>
      <td>0.411</td>
      <td>MSA Transformer (single MSA sample)</td>
      <td><a href='http://proceedings.mlr.press/v139/rao21a.html'>Rao, R., Liu, J., Verkuil, R., Meier, J., Canny, J.F., Abbeel, P., Sercu, T., & Rives, A. (2021). MSA Transformer. ICML.</a></td>
    </tr>
    <tr>
      <th>11</th>
      <td>Tranception S</td>
      <td>Hybrid model</td>
      <td>0.419</td>
      <td>0.009</td>
      <td>0.436</td>
      <td>0.410</td>
      <td>0.429</td>
      <td>0.404</td>
      <td>0.477</td>
      <td>0.418</td>
      <td>0.413</td>
      <td>0.415</td>
      <td>0.329</td>
      <td>0.278</td>
      <td>0.3</td>
      <td>0.403</td>
      <td>Tranception Small model (85M params) with retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>12</th>
      <td>EVmutation</td>
      <td>Alignment-based model</td>
      <td>0.413</td>
      <td>0.005</td>
      <td>0.404</td>
      <td>0.405</td>
      <td>0.444</td>
      <td>0.385</td>
      <td>0.448</td>
      <td>0.472</td>
      <td>0.381</td>
      <td>0.414</td>
      <td>0.401</td>
      <td>0.403</td>
      <td>0.335</td>
      <td>0.427</td>
      <td>EVmutation model</td>
      <td><a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., SchÃ¤rfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a></td>
    </tr>
    <tr>
      <th>13</th>
      <td>Progen2 (ensemble)</td>
      <td>Protein language model</td>
      <td>0.413</td>
      <td>0.011</td>
      <td>0.367</td>
      <td>0.416</td>
      <td>0.447</td>
      <td>0.399</td>
      <td>0.459</td>
      <td>0.462</td>
      <td>0.364</td>
      <td>0.41</td>
      <td>0.312</td>
      <td>0.233</td>
      <td>0.228</td>
      <td>0.282</td>
      <td>Ensemble of the 5 Progen2 models</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>14</th>
      <td>VESPAl</td>
      <td>Protein language model</td>
      <td>0.408</td>
      <td>0.010</td>
      <td>0.401</td>
      <td>0.386</td>
      <td>0.476</td>
      <td>0.370</td>
      <td>0.472</td>
      <td>0.487</td>
      <td>0.356</td>
      <td>0.399</td>
      <td>0.359</td>
      <td>0.313</td>
      <td>0.26</td>
      <td>0.328</td>
      <td>VESPAl model</td>
      <td><a href='https://link.springer.com/article/10.1007/s00439-021-02411-y'>Marquet, C., Heinzinger, M., Olenyi, T., Dallago, C., Bernhofer, M., Erckert, K., & Rost, B. (2021). Embeddings from protein language models predict conservation and variant effects. Human Genetics, 141, 1629 - 1647.</a></td>
    </tr>
    <tr>
      <th>15</th>
      <td>DeepSequence (single)</td>
      <td>Alignment-based model</td>
      <td>0.404</td>
      <td>0.009</td>
      <td>0.389</td>
      <td>0.388</td>
      <td>0.464</td>
      <td>0.392</td>
      <td>0.478</td>
      <td>0.465</td>
      <td>0.326</td>
      <td>0.398</td>
      <td>0.357</td>
      <td>0.337</td>
      <td>0.315</td>
      <td>0.412</td>
      <td>DeepSequence model (single seed)</td>
      <td><a href='https://www.nature.com/articles/s41592-018-0138-4'>Riesselman, A.J., Ingraham, J., & Marks, D.S. (2018). Deep generative models of genetic variation capture the effects of mutations. Nature Methods, 15, 816-822.</a></td>
    </tr>
    <tr>
      <th>16</th>
      <td>Progen2 XL</td>
      <td>Protein language model</td>
      <td>0.402</td>
      <td>0.010</td>
      <td>0.362</td>
      <td>0.401</td>
      <td>0.447</td>
      <td>0.342</td>
      <td>0.466</td>
      <td>0.490</td>
      <td>0.376</td>
      <td>0.403</td>
      <td>0.358</td>
      <td>0.382</td>
      <td>0.315</td>
      <td>0.348</td>
      <td>Progen2 xlarge model (6.4B params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>17</th>
      <td>Tranception L no retrieval</td>
      <td>Protein language model</td>
      <td>0.401</td>
      <td>0.009</td>
      <td>0.379</td>
      <td>0.399</td>
      <td>0.429</td>
      <td>0.361</td>
      <td>0.436</td>
      <td>0.450</td>
      <td>0.395</td>
      <td>0.393</td>
      <td>0.398</td>
      <td>0.418</td>
      <td>0.334</td>
      <td>0.422</td>
      <td>Tranception Large model (700M params) without retrieval</td>
      <td><a href='https://proceedings.mlr.press/v162/notin22a.html'>Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.</a></td>
    </tr>
    <tr>
      <th>18</th>
      <td>ESM-1v (ensemble)</td>
      <td>Protein language model</td>
      <td>0.401</td>
      <td>0.019</td>
      <td>0.372</td>
      <td>0.372</td>
      <td>0.510</td>
      <td>0.424</td>
      <td>0.441</td>
      <td>0.502</td>
      <td>0.256</td>
      <td>0.399</td>
      <td>0.309</td>
      <td>0.203</td>
      <td>0.165</td>
      <td>0.253</td>
      <td>ESM-1v (ensemble of 5 independently-trained models)</td>
      <td><a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a></td>
    </tr>
    <tr>
      <th>19</th>
      <td>Wavenet</td>
      <td>Alignment-based model</td>
      <td>0.391</td>
      <td>0.010</td>
      <td>0.312</td>
      <td>0.396</td>
      <td>0.457</td>
      <td>0.378</td>
      <td>0.446</td>
      <td>0.472</td>
      <td>0.308</td>
      <td>0.389</td>
      <td>0.344</td>
      <td>0.324</td>
      <td>0.282</td>
      <td>0.35</td>
      <td>Wavenet model</td>
      <td><a href='https://www.nature.com/articles/s41467-021-22732-w'>Shin, J., Riesselman, A.J., Kollasch, A.W., McMahon, C., Simon, E., Sander, C., Manglik, A., Kruse, A.C., & Marks, D.S. (2021). Protein design and variant prediction using autoregressive generative models. Nature Communications, 12.</a></td>
    </tr>
    <tr>
      <th>20</th>
      <td>RITA (ensemble)</td>
      <td>Protein language model</td>
      <td>0.388</td>
      <td>0.014</td>
      <td>0.332</td>
      <td>0.409</td>
      <td>0.385</td>
      <td>0.378</td>
      <td>0.387</td>
      <td>0.381</td>
      <td>0.410</td>
      <td>0.38</td>
      <td>0.236</td>
      <td>0.135</td>
      <td>0.181</td>
      <td>0.246</td>
      <td>Ensemble of the 4 RITA models</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>21</th>
      <td>Progen2 L</td>
      <td>Protein language model</td>
      <td>0.387</td>
      <td>0.012</td>
      <td>0.362</td>
      <td>0.385</td>
      <td>0.417</td>
      <td>0.380</td>
      <td>0.444</td>
      <td>0.434</td>
      <td>0.325</td>
      <td>0.387</td>
      <td>0.333</td>
      <td>0.281</td>
      <td>0.265</td>
      <td>0.302</td>
      <td>Progen2 large model (2.7B params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>22</th>
      <td>Progen2 M</td>
      <td>Protein language model</td>
      <td>0.383</td>
      <td>0.014</td>
      <td>0.327</td>
      <td>0.393</td>
      <td>0.413</td>
      <td>0.380</td>
      <td>0.406</td>
      <td>0.428</td>
      <td>0.337</td>
      <td>0.383</td>
      <td>0.274</td>
      <td>0.167</td>
      <td>0.173</td>
      <td>0.22</td>
      <td>Progen2 medium model (760M params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>23</th>
      <td>Progen2 Base</td>
      <td>Protein language model</td>
      <td>0.383</td>
      <td>0.013</td>
      <td>0.356</td>
      <td>0.388</td>
      <td>0.396</td>
      <td>0.383</td>
      <td>0.424</td>
      <td>0.422</td>
      <td>0.327</td>
      <td>0.378</td>
      <td>0.25</td>
      <td>0.133</td>
      <td>0.18</td>
      <td>0.246</td>
      <td>Progen2 base model (760M params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>24</th>
      <td>RITA XL</td>
      <td>Protein language model</td>
      <td>0.380</td>
      <td>0.013</td>
      <td>0.309</td>
      <td>0.400</td>
      <td>0.394</td>
      <td>0.352</td>
      <td>0.382</td>
      <td>0.404</td>
      <td>0.398</td>
      <td>0.376</td>
      <td>0.234</td>
      <td>0.151</td>
      <td>0.193</td>
      <td>0.26</td>
      <td>RITA xlarge model (1.2B params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>25</th>
      <td>Site-Independent</td>
      <td>Alignment-based model</td>
      <td>0.375</td>
      <td>0.012</td>
      <td>0.434</td>
      <td>0.378</td>
      <td>0.309</td>
      <td>0.366</td>
      <td>0.417</td>
      <td>0.324</td>
      <td>0.412</td>
      <td>0.375</td>
      <td>0.322</td>
      <td>0.285</td>
      <td>0.321</td>
      <td>0.407</td>
      <td>Site-Independent model</td>
      <td><a href='https://www.nature.com/articles/nbt.3769'>Hopf, T.A., Ingraham, J., Poelwijk, F.J., SchÃ¤rfe, C.P., Springer, M., Sander, C., & Marks, D.S. (2017). Mutation effects predicted from sequence co-variation. Nature Biotechnology, 35, 128-135.</a></td>
    </tr>
    <tr>
      <th>26</th>
      <td>RITA L</td>
      <td>Protein language model</td>
      <td>0.375</td>
      <td>0.014</td>
      <td>0.322</td>
      <td>0.396</td>
      <td>0.365</td>
      <td>0.367</td>
      <td>0.389</td>
      <td>0.354</td>
      <td>0.398</td>
      <td>0.362</td>
      <td>0.227</td>
      <td>0.145</td>
      <td>0.179</td>
      <td>0.242</td>
      <td>RITA large model (680M params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>27</th>
      <td>ESM-1v (single)</td>
      <td>Protein language model</td>
      <td>0.372</td>
      <td>0.020</td>
      <td>0.329</td>
      <td>0.350</td>
      <td>0.480</td>
      <td>0.396</td>
      <td>0.417</td>
      <td>0.480</td>
      <td>0.220</td>
      <td>0.374</td>
      <td>0.29</td>
      <td>0.203</td>
      <td>0.151</td>
      <td>0.235</td>
      <td>ESM-1v (single seed)</td>
      <td><a href='https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html'>Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., & Rives, A. (2021). Language models enable zero-shot prediction of the effects of mutations on protein function. NeurIPS.</a></td>
    </tr>
    <tr>
      <th>28</th>
      <td>RITA M</td>
      <td>Protein language model</td>
      <td>0.366</td>
      <td>0.015</td>
      <td>0.324</td>
      <td>0.388</td>
      <td>0.345</td>
      <td>0.358</td>
      <td>0.375</td>
      <td>0.349</td>
      <td>0.388</td>
      <td>0.36</td>
      <td>0.237</td>
      <td>0.122</td>
      <td>0.167</td>
      <td>0.223</td>
      <td>RITA medium model (300M params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>29</th>
      <td>ESM-1b</td>
      <td>Protein language model</td>
      <td>0.358</td>
      <td>0.019</td>
      <td>0.344</td>
      <td>0.327</td>
      <td>0.463</td>
      <td>0.391</td>
      <td>0.442</td>
      <td>0.485</td>
      <td>0.153</td>
      <td>0.351</td>
      <td>0.318</td>
      <td>0.215</td>
      <td>0.161</td>
      <td>0.318</td>
      <td>ESM-1b (w/ Brandes et al. extensions)</td>
      <td>[1] Original model: <a href='https://www.biorxiv.org/content/10.1101/622803v4'>Rives, A., Goyal, S., Meier, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences of the United States of America, 118.</a> [2] Extensions: <a href='https://www.biorxiv.org/content/10.1101/2022.08.25.505311v1'>Brandes, N., Goldman, G., Wang, C.H., Ye, C.J., & Ntranos, V. (2022). Genome-wide prediction of disease variants with a deep protein language model. bioRxiv.</a></td>
    </tr>
    <tr>
      <th>30</th>
      <td>Unirep evotuned</td>
      <td>Hybrid model</td>
      <td>0.348</td>
      <td>0.014</td>
      <td>0.314</td>
      <td>0.351</td>
      <td>0.373</td>
      <td>0.334</td>
      <td>0.338</td>
      <td>0.372</td>
      <td>0.351</td>
      <td>0.339</td>
      <td>0.284</td>
      <td>0.279</td>
      <td>0.246</td>
      <td>0.323</td>
      <td>Unirep model w/ evotuning</td>
      <td><a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a></td>
    </tr>
    <tr>
      <th>31</th>
      <td>Progen2 S</td>
      <td>Protein language model</td>
      <td>0.341</td>
      <td>0.019</td>
      <td>0.307</td>
      <td>0.345</td>
      <td>0.366</td>
      <td>0.371</td>
      <td>0.374</td>
      <td>0.361</td>
      <td>0.264</td>
      <td>0.334</td>
      <td>0.249</td>
      <td>0.118</td>
      <td>0.161</td>
      <td>0.202</td>
      <td>Progen2 small model (150M params)</td>
      <td><a href='https://arxiv.org/abs/2206.13517'> Nijkamp, E., Ruffolo, J.A., Weinstein, E.N., Naik, N., & Madani, A. (2022). ProGen2: Exploring the Boundaries of Protein Language Models. ArXiv, abs/2206.13517. </a></td>
    </tr>
    <tr>
      <th>32</th>
      <td>RITA S</td>
      <td>Protein language model</td>
      <td>0.321</td>
      <td>0.017</td>
      <td>0.278</td>
      <td>0.344</td>
      <td>0.299</td>
      <td>0.310</td>
      <td>0.311</td>
      <td>0.290</td>
      <td>0.370</td>
      <td>0.315</td>
      <td>0.211</td>
      <td>0.082</td>
      <td>0.137</td>
      <td>0.181</td>
      <td>RITA small model (85M params)</td>
      <td><a href='https://arxiv.org/abs/2205.05789'>Hesslow, D., Zanichelli, N., Notin, P., Poli, I., & Marks, D.S. (2022). RITA: a Study on Scaling Up Generative Protein Sequence Models. ArXiv, abs/2205.05789.</a></td>
    </tr>
    <tr>
      <th>33</th>
      <td>ProtGPT2</td>
      <td>Protein language model</td>
      <td>0.175</td>
      <td>0.019</td>
      <td>0.167</td>
      <td>0.183</td>
      <td>0.161</td>
      <td>0.218</td>
      <td>0.172</td>
      <td>0.168</td>
      <td>0.122</td>
      <td>0.176</td>
      <td>0.196</td>
      <td>0.057</td>
      <td>0.052</td>
      <td>0.067</td>
      <td>ProtGPT2 model</td>
      <td><a href='https://www.nature.com/articles/s41467-022-32007-7'>Ferruz, N., Schmidt, S., & HÃ¶cker, B. (2022). ProtGPT2 is a deep unsupervised language model for protein design. Nature Communications, 13.</a></td>
    </tr>
    <tr>
      <th>34</th>
      <td>Unirep</td>
      <td>Protein language model</td>
      <td>0.166</td>
      <td>0.023</td>
      <td>0.224</td>
      <td>0.149</td>
      <td>0.159</td>
      <td>0.254</td>
      <td>0.226</td>
      <td>0.166</td>
      <td>0.010</td>
      <td>0.161</td>
      <td>0.201</td>
      <td>0.097</td>
      <td>0.126</td>
      <td>0.192</td>
      <td>Unirep model</td>
      <td><a href='https://www.nature.com/articles/s41592-019-0598-1'>Alley, E.C., Khimulya, G., Biswas, S., AlQuraishi, M., & Church, G.M. (2019). Unified rational protein engineering with sequence-based deep representation learning. Nature Methods, 1-8.</a></td>
    </tr>
  </tbody>
                    
                    
                    </table>
				</div>
			</div>
		</div>
	</div>
	<div class="fw-footer">
        <br>
        <div class="skew"> </div>
        <div class="skew-bg"></div>
            <font size="2"><sup>*</sup> Non-parametric bootstrap standard error of the difference between the Spearman performance of a given model and that of the best overall model (ie., TranceptEVE), computed over 10k bootstrap samples from the set of proteins in the ProteinGym substitution benchmark.</font>
	</div>
	<script type="text/javascript">
				  var _gaq = _gaq || [];
				  _gaq.push(['_setAccount', 'UA-365466-5']);
				  _gaq.push(['_trackPageview']);

				  (function() {
					var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
					ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
					var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
				  })();
	</script>
</body>
</html>